import ioimport jsonimport sysimport osimport timefrom configparser import ConfigParser# 尝试设置标准输出编码，但添加异常处理try:	if hasattr(sys.stdout, 'reconfigure'):		if sys.stdout.encoding != 'utf-8':			sys.stdout.reconfigure(encoding='utf-8')		if sys.stderr.encoding != 'utf-8':			sys.stderr.reconfigure(encoding='utf-8')except (AttributeError, OSError, io.UnsupportedOperation):	# 在某些环境下reconfigure可能不可用，忽略错误	pass# 数据库配置try:	import mysql.connector	from mysql.connector import Error	MYSQL_AVAILABLE = Trueexcept ImportError:	MYSQL_AVAILABLE = False	mysql = None	Error = Exception	print("MySQL connector not available, using default configuration", file=sys.stderr)# 获取当前脚本所在目录def get_resources_dir():	"""获取资源目录路径"""	if getattr(sys, 'frozen', False):		# 如果是打包后的exe文件，使用可执行文件所在目录		resources_dir = os.path.dirname(sys.executable)		# 如果resources目录不存在，则创建它		return resources_dir	else:		# 如果是普通的Python脚本		current_dir = os.path.dirname(os.path.abspath(__file__))		# 检查是否在标准项目结构中(src/main/python)		src_main_python_path = os.path.join('src', 'main', 'python')		if current_dir.endswith(src_main_python_path):			# 构建到正确resources目录的路径 (src/main/resources)			src_dir = os.path.dirname(os.path.dirname(current_dir))  # src/			main_dir = os.path.join(src_dir, 'main')  # src/main			return os.path.join(main_dir, 'resources')      # src/main/resources		else:			# 如果不在标准项目结构中，就在当前目录下查找resources文件夹			return os.path.join(current_dir, 'resources')resources_dir = get_resources_dir()NLTK_DATA_PATH = os.path.join(resources_dir, 'nltk_data')  # resources/nltk_data# 读取数据库配置文件def read_db_config(filename='database.ini', section='mysql'):	"""	从配置文件中读取数据库配置	"""	# 默认数据库配置	default_config = {		'host': 'localhost',		'database': 'chatbot_data',		'user': 'ChatBot',		'password': '123456'	}		parser = ConfigParser()	config_path = os.path.join(resources_dir, filename)		# 检查配置文件是否存在	if not os.path.exists(config_path):		if 'permitLogPrint' in globals() and permitLogPrint:			print(f"Configuration file {config_path} not found, using default database configuration", file=sys.stderr)		return default_config		try:		parser.read(config_path, encoding='utf-8')	except Exception as ex:		if 'permitLogPrint' in globals() and permitLogPrint:			print(f"Error reading config file {config_path}: {ex}, using default database configuration", file=sys.stderr)		return default_config		db_config = {}	if parser.has_section(section):		items = parser.items(section)		for item in items:			db_config[item[0]] = item[1]		return db_config	else:		if 'permitLogPrint' in globals() and permitLogPrint:			print(f'Section {section} not found in {filename}, using default database configuration', file=sys.stderr)		return default_config# 从本地配置文件读取配置def load_config_from_file():	"""	从本地JSON配置文件加载配置	"""	config = {		'permitLogPrint': False  # 默认值	}		config_path = os.path.join(resources_dir, 'app_config.json')		# 确保资源目录存在	try:		os.makedirs(resources_dir, exist_ok=True)	except Exception as e:		print(f"Error creating resources directory {resources_dir}: {e}", file=sys.stderr)	# 检查配置文件是否存在	if os.path.exists(config_path):		try:			with open(config_path, 'r', encoding='utf-8') as f:				file_config = json.load(f)				config.update(file_config)		except Exception as ex:			print(f"Error reading config file: {ex}", file=sys.stderr)			print("Using default configuration", file=sys.stderr)	else:		# 如果配置文件不存在，创建一个默认配置文件		try:			with open(config_path, 'w', encoding='utf-8') as f:				json.dump(config, f, ensure_ascii=False, indent=4)			print(f"Created default config file: {config_path}", file=sys.stderr)		except Exception as ex:			print(f"Error creating default config file: {ex}", file=sys.stderr)		return config# 从数据库读取配置def load_config_from_db():	"""	从MySQL数据库加载配置	"""	# 首先尝试从本地配置文件加载	config = load_config_from_file()		# 如果MySQL不可用，直接返回文件配置	if not MYSQL_AVAILABLE:		if config.get('permitLogPrint', True):			print("MySQL connector not available, using local config file", file=sys.stderr)		return config		connection = None	cursor = None		try:		# 从配置文件读取数据库连接信息		db_config = read_db_config()				# 数据库连接		connection = mysql.connector.connect(**db_config)				if connection.is_connected():			cursor = connection.cursor()			# 查询所有配置项			cursor.execute("SELECT config_key, config_value FROM app_config")			records = cursor.fetchall()						# 更新配置			for key, value in records:				# 将字符串转换为适当的类型				if key == 'permitLogPrint':					config[key] = value.lower() == 'true'				else:					config[key] = value					except Error as ex:		if config.get('permitLogPrint', True):			# 使用英文输出避免编码问题			error_msg = f"Database connection error"			if hasattr(ex, 'errno'):				error_msg += f": {ex.errno}"			if hasattr(ex, 'sqlstate'):				error_msg += f" ({ex.sqlstate})"			if hasattr(ex, 'msg'):				error_msg += f": {ex.msg}"			print(error_msg, file=sys.stderr)			print("Using local config file", file=sys.stderr)	except Exception as ex:		if config.get('permitLogPrint', True):			print(f"Config reading error: {ex}", file=sys.stderr)			print("Using local config file", file=sys.stderr)	finally:		# 关闭数据库连接		if connection and connection.is_connected():			if cursor:				cursor.close()			connection.close()				return config# 初始化permitPrint为True，以便在加载配置时可以打印信息permitLogPrint = True# 确保资源目录存在if not os.path.exists(resources_dir):	try:		os.makedirs(resources_dir, exist_ok=True)	except Exception as e:		print(f"Error creating resources directory {resources_dir}: {e}", file=sys.stderr)# 加载配置try:	app_config = load_config_from_db()	permitLogPrint = app_config['permitLogPrint']except Exception as e:	print(f"Error loading configuration: {e}, using default configuration", file=sys.stderr)	permitLogPrint = True  # 使用默认值# 检查目录是否存在if os.path.exists(NLTK_DATA_PATH) and permitLogPrint:	print(f"Using existing NLTK data directory: {NLTK_DATA_PATH}", file=sys.stderr)else:	if permitLogPrint:		print(f"NLTK data directory not found, will create: {NLTK_DATA_PATH}", file=sys.stderr)	# 如果目录不存在则创建	os.makedirs(NLTK_DATA_PATH, exist_ok=True)# 导入nltk之前先配置路径import nltk# 清空现有路径并只使用指定路径nltk.data.path.clear()nltk.data.path.append(NLTK_DATA_PATH)# 禁用自动下载提示nltk.download_gui = lambda: Nonefrom nltk.chunk import ne_chunkfrom nltk.sentiment import SentimentIntensityAnalyzerfrom nltk.stem import WordNetLemmatizerfrom nltk.tag import pos_tagfrom nltk.tokenize import word_tokenize, sent_tokenize# 自动下载所需的NLTK数据def download_nltk_data():	"""	下载并验证所需的NLTK数据包	检查并下载缺失的NLTK数据包，包括分词器、词性标注器、命名实体识别器等	所需的数据包：punkt(分词), averaged_perceptron_tagger(词性标注),	maxent_ne_chunker(命名实体识别), words(词汇表), vader_lexicon(情感词典),	wordnet(词形还原)	"""	# 确保目录存在	def ensure_directory_exists(directory_path):		"""确保目录存在，如果不存在则创建"""		try:			os.makedirs(directory_path, exist_ok=True)		except Exception as e:			print(f"Error creating directory {directory_path}: {e}", file=sys.stderr)		ensure_directory_exists(NLTK_DATA_PATH)		# 设置NLTK数据路径	nltk.data.path.clear()	nltk.data.path.append(NLTK_DATA_PATH)		required_data = [		('punkt', 'tokenizers/punkt'),		('punkt_tab', 'tokenizers/punkt_tab'),		('averaged_perceptron_tagger', 'taggers/averaged_perceptron_tagger'),		('averaged_perceptron_tagger_eng', 'taggers/averaged_perceptron_tagger_eng'),		('maxent_ne_chunker', 'chunkers/maxent_ne_chunker'),		('maxent_ne_chunker_tab', 'chunkers/maxent_ne_chunker_tab'),		('words', 'corpora/words'),		('vader_lexicon', 'corpora/vader_lexicon'),		('wordnet', 'corpora/wordnet')	]	# 创建标记文件路径	downloaded_flag_file = os.path.join(NLTK_DATA_PATH, '.downloaded')	downloaded_packages = set()		# 如果标记文件存在，读取已下载的包	if os.path.exists(downloaded_flag_file):		try:			with open(downloaded_flag_file, 'r') as f:				downloaded_packages = set(f.read().splitlines())		except IOError:			pass		packages_to_download = []	for package, path in required_data:		if package in downloaded_packages:			# 检查资源是否真的存在			try:				nltk.data.find(path)				continue  # 已经下载且能找到，跳过			except LookupError:				# 标记为已下载但实际上找不到，需要重新下载				pass				# 需要下载的包		packages_to_download.append((package, path))		# 只有真正需要下载的才进行下载	if packages_to_download:		if permitLogPrint:			print(f"Need to download {len(packages_to_download)} packages", file=sys.stderr)		newly_downloaded = set()		for package, path in packages_to_download:			try:				if permitLogPrint:					print(f"Downloading {package}...", file=sys.stderr)				# 添加重试机制以解决文件锁定问题				retry_count = 5  # 增加重试次数				while retry_count > 0:					try:						# 下载到指定目录						nltk.download(package, download_dir=NLTK_DATA_PATH, quiet=True)						newly_downloaded.add(package)						break					except Exception as ex:						retry_count -= 1						if retry_count <= 0:							raise ex						else:							if permitLogPrint:								print(f"Download failed, retrying... ({retry_count} attempts left)", file=sys.stderr)							# 增加等待时间并使用随机延迟以减少冲突							time.sleep(2 + retry_count * 0.5)			except Exception as ex:				print(f"Failed to download {package}: {ex}", file=sys.stderr)				# 即使某个包下载失败，也继续下载其他包				# 更新标记文件		if newly_downloaded:			downloaded_packages.update(newly_downloaded)			try:				with open(downloaded_flag_file, 'w') as f:					f.write('\n'.join(downloaded_packages))			except IOError:				pass	elif permitLogPrint:		print("All required NLTK data packages are already downloaded", file=sys.stderr)# 缓存NLP处理器实例_nlp_processor_instance = Nonedef get_nlp_processor():	"""获取单例NLP处理器实例"""	global _nlp_processor_instance	if _nlp_processor_instance is None:		_nlp_processor_instance = SimpleNLPProcessor()	return _nlp_processor_instancedownload_nltk_data()class SimpleNLPProcessor:	"""	简单的自然语言处理类	提供多种基础的NLP功能，包括分词、词性标注、情感分析、命名实体识别、	词形还原和简单意图识别等功能	"""	def __init__(self):		"""		初始化NLP处理器		创建WordNet词形还原器和VADER情感分析器实例		"""		self.lemmatizer = WordNetLemmatizer()		self.sentiment_analyzer = SentimentIntensityAnalyzer()	@staticmethod	def tokenize_text(input_text):		"""		对文本进行分词处理		Args:			input_text (str): 需要分词的文本		Returns:			dict: 包含分词结果、句子数和单词数的字典				- tokens (list): 分词后的单词列表				- sentence_count (int): 句子数量				- word_count (int): 单词数量		"""		tokens = word_tokenize(input_text)		sentences = sent_tokenize(input_text)		return {			'tokens': tokens,			'sentence_count': len(sentences),			'word_count': len(tokens)		}	@staticmethod	def pos_tagging(input_text):		"""		对文本进行词性标注		Args:			input_text (str): 需要进行词性标注的文本		Returns:			dict: 包含标注后单词及其词性的字典				- tagged_words (list): 包含单词和对应词性的对象列表		"""		tokens = word_tokenize(input_text)		tagged = pos_tag(tokens)		return {			'tagged_words': [{'word': word, 'pos': pos} for word, pos in tagged]		}	def sentiment_analysis(self, input_text):		"""		对文本进行情感分析		使用VADER情感分析器分析文本的情感倾向		Args:			input_text (str): 需要进行情感分析的文本		Returns:			dict: 包含各种情感分数和整体情感分类的字典				- compound (float): 复合情感分数 (-1到1)				- positive (float): 积极情感分数 (0到1)				- negative (float): 消极情感分数 (0到1)				- neutral (float): 中性情感分数 (0到1)				- sentiment (str): 整体情感分类 ('positive', 'negative', 'neutral')		"""		scores = self.sentiment_analyzer.polarity_scores(input_text)		return {			'compound': scores['compound'],			'positive': scores['pos'],			'negative': scores['neg'],			'neutral': scores['neu'],			'sentiment': 'positive' if scores['compound'] > 0.05 else 'negative' if scores['compound'] < -0.05 else 'neutral'		}	@staticmethod	def named_entity_recognition(input_text):		"""		对文本进行命名实体识别		Args:			input_text (str): 需要进行命名实体识别的文本		Returns:			dict: 包含识别出的命名实体的字典				- entities (list): 命名实体列表，每个实体包含文本和类型信息		"""		tokens = word_tokenize(input_text)		tagged = pos_tag(tokens)		entities = ne_chunk(tagged)		entity_list = []		for entity in entities:			if hasattr(entity, 'label'):				entity_text = ' '.join([word for word, pos in entity.leaves()])				entity_list.append({					'text': entity_text,					'type': entity.label()				})		return {'entities': entity_list}	def lemmatize_text(self, input_text):		"""		对文本进行词形还原		Args:			input_text (str): 需要进行词形还原的文本		Returns:			dict: 包含词形还原后单词的字典				- lemmas (list): 还原后的词根形式单词列表		"""		tokens = word_tokenize(input_text)		lemmas = [self.lemmatizer.lemmatize(token) for token in tokens]		return {'lemmas': lemmas}	@staticmethod	def intent_recognition(input_text):		"""		对文本进行简单的意图识别		通过关键词匹配来识别文本的基本意图类别		Args:			input_text (str): 需要进行意图识别的文本		Returns:			dict: 包含识别出的意图和置信度的字典				- intents (list): 识别出的所有意图列表				- primary_intent (str): 主要意图				- confidence (float): 置信度分数		"""		text_lower = input_text.lower()		tokens = [token.lower() for token in word_tokenize(text_lower)]		# 简单的规则匹配		intents = []		confidence = 0.0		greeting_words = ['hello', 'hi', 'hey', 'greetings']		question_words = ['what', 'when', 'where', 'why', 'how', 'who', 'which']		farewell_words = ['bye', 'goodbye', 'see you', 'farewell']		if any(word in tokens for word in greeting_words):			intents.append('greeting')			confidence = max(confidence, 0.8)		if any(word in tokens for word in question_words) or '?' in input_text:			intents.append('question')			confidence = max(confidence, 0.7)		if any(word in tokens for word in farewell_words):			intents.append('farewell')			confidence = max(confidence, 0.9)		if not intents:			intents.append('statement')			confidence = 0.5		return {			'intents': intents,			'primary_intent': intents[0],			'confidence': confidence		}# 主处理函数def process_request(request_method, input_text):	"""	处理NLP请求	根据指定的方法对文本执行相应的NLP处理	Args:		request_method (str): 要执行的NLP方法名称		input_text (str): 要处理的文本	Returns:		dict: 处理结果或错误信息	"""	# 使用单例模式获取处理器实例	processor = get_nlp_processor()	try:		method_map = {			'tokenize': processor.tokenize_text,			'pos_tag': processor.pos_tagging,			'sentiment': processor.sentiment_analysis,			'ner': processor.named_entity_recognition,			'lemmatize': processor.lemmatize_text,			'intent': processor.intent_recognition		}		if request_method in method_map:			response = method_map[request_method](input_text)		else:			return {'status': 'error', 'error': f'Unknown method: {request_method}'}		response['status'] = 'success'		return response	except Exception as ex:		return {'status': 'error', 'error': str(ex)}def handle_persistent_mode():	"""	处理持久化模式通信	从标准输入读取请求，处理后将结果写入标准输出	请求格式: method|||text	"""	for line in sys.stdin:		line = line.strip()		if not line:			continue		try:			# 解析请求格式: method|||text			parts = line.split('|||', 1)			if len(parts) != 2:				response_data = {'status': 'error', 'error': 'Invalid request format. Expected: method|||text'}			else:				request_method, input_text = parts				response_data = process_request(request_method, input_text)		except Exception as ex:			# 捕获所有异常并返回错误信息，防止进程崩溃			response_data = {'status': 'error', 'error': f'Python script error: {str(ex)}', 'exception_type': type(ex).__name__}		try:			# 输出结果为JSON格式			print(json.dumps(response_data))			sys.stdout.flush()		except Exception as ex:			# 如果输出也有问题，至少记录到stderr			print(f"Error writing output: {ex}", file=sys.stderr)			sys.stdout.flush()def test_nlp_functions():	"""测试所有NLP功能"""	print("开始测试NLP功能...")		# 测试文本	test_text = "Hello! How are you today? I'm doing well. Goodbye!"		# 测试分词	print("\n1. 测试分词功能:")	result = process_request('tokenize', test_text)	print(f"分词结果: {result}")		# 测试词性标注	print("\n2. 测试词性标注功能:")	result = process_request('pos_tag', test_text)	print(f"词性标注结果: {result}")		# 测试情感分析	print("\n3. 测试情感分析功能:")	result = process_request('sentiment', test_text)	print(f"情感分析结果: {result}")		# 测试命名实体识别	print("\n4. 测试命名实体识别功能:")	result = process_request('ner', test_text)	print(f"命名实体识别结果: {result}")		# 测试词形还原	print("\n5. 测试词形还原功能:")	result = process_request('lemmatize', test_text)	print(f"词形还原结果: {result}")		# 测试意图识别	print("\n6. 测试意图识别功能:")	result = process_request('intent', test_text)	print(f"意图识别结果: {result}")		print("\n测试完成!")if __name__ == "__main__":	"""	程序入口点		如果没有命令行参数，则进入持久化模式等待输入；	如果有命令行参数，则按传统方式处理单次请求。	"""	# test_nlp_functions()	# 检查是否有命令行参数	if len(sys.argv) == 1:		# 没有参数，进入持久化模式		handle_persistent_mode()	elif len(sys.argv) == 3:		# 有正确数量的参数，按传统方式处理		method = sys.argv[1]		text = sys.argv[2]		result = process_request(method, text)		print(json.dumps(result))	else:		# 参数数量不正确		print(json.dumps({'status': 'error', 'error': 'Usage: python nlp_processor.py <method> <text>'}))		sys.exit(1)